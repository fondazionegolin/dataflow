# ğŸš€ Quick Reference: Modelli LLM per RTX 4090

## âš ï¸ SOLO MODELLI OPEN ACCESS (No Login HuggingFace)

Tutti i modelli in questa lista sono **open access** e non richiedono autorizzazione.

---

## Top 5 Modelli Consigliati

### 1. ğŸ¥‡ **Qwen2.5-7B** - Migliore per Italiano
```
Model: Qwen/Qwen2.5-7B
VRAM: ~14GB | QualitÃ : â­â­â­â­â­ | Italiano: âœ…âœ…
```
**Quando usarlo**: Progetti in italiano, multilingua, alta qualitÃ 

### 2. ğŸ¥ˆ **Phi-2** - Migliore Efficienza
```
Model: microsoft/phi-2
VRAM: ~6GB | QualitÃ : â­â­â­â­ | VelocitÃ : âš¡âš¡âš¡
```
**Quando usarlo**: Sperimentazione, test rapidi, risorse limitate

### 3. ğŸ¥‰ **Qwen2.5-1.5B** - Bilanciato
```
Model: Qwen/Qwen2.5-1.5B
VRAM: ~4GB | QualitÃ : â­â­â­ | VelocitÃ : âš¡âš¡âš¡
```
**Quando usarlo**: Buon compromesso qualitÃ /velocitÃ 

### 4. **TinyLlama-1.1B** - Ultra Veloce
```
Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0
VRAM: ~3GB | QualitÃ : â­â­â­ | VelocitÃ : âš¡âš¡âš¡
```
**Quando usarlo**: Test rapidi, risorse molto limitate

### 5. **StableLM-2-1.6B** - Alternativa Stabile
```
Model: stabilityai/stablelm-2-1_6b
VRAM: ~4GB | QualitÃ : â­â­â­ | VelocitÃ : âš¡âš¡âš¡
```
**Quando usarlo**: Alternativa a Qwen per progetti piccoli

---

## âš™ï¸ Impostazioni Rapide

### Setup Veloce (Phi-2)
```yaml
Batch Size: 4
Gradient Accumulation: 4
Max Length: 128
Epochs: 3
```
â±ï¸ ~5-10 min per 1000 esempi

### Setup QualitÃ  (Qwen2.5-7B)
```yaml
Batch Size: 1
Gradient Accumulation: 16
Max Length: 128
Epochs: 3
```
â±ï¸ ~20-30 min per 1000 esempi

### Setup Massimo (Qwen2.5-7B)
```yaml
Batch Size: 1
Gradient Accumulation: 16
Max Length: 256
Epochs: 2
```
â±ï¸ ~30-40 min per 1000 esempi

---

## ğŸ¯ Scegli per Caso d'Uso

| Caso d'Uso | Modello Consigliato | Accesso |
|------------|---------------------|----------|
| ğŸ‡®ğŸ‡¹ **Italiano** | Qwen/Qwen2.5-7B | âœ… Open |
| ğŸ’¬ **Chatbot** | Qwen/Qwen2.5-7B | âœ… Open |
| ğŸ’» **Codice** | microsoft/phi-2 | âœ… Open |
| âš¡ **VelocitÃ ** | microsoft/phi-2 | âœ… Open |
| ğŸ“ **Scrittura** | Qwen/Qwen2.5-7B | âœ… Open |
| ğŸ§ª **Test** | TinyLlama/TinyLlama-1.1B-Chat-v1.0 | âœ… Open |

---

## ğŸ†˜ Problemi Comuni

### âŒ CUDA Out of Memory
1. Batch Size â†’ 1
2. Max Length â†’ 64
3. Usa modello piÃ¹ piccolo

### â³ Troppo Lento
1. Usa Phi-2 o TinyLlama
2. Riduci Max Length
3. Riduci Epochs

### ğŸ”’ Model Gated (403 Error)
1. âŒ Modello rimosso dalla lista (richiede autorizzazione)
2. âœ… Usa alternative open access: Qwen2.5-7B, phi-2, TinyLlama

---

## ğŸ“Š Confronto Veloce

| Modello | Parametri | VRAM | QualitÃ  | Italiano | Accesso |
|---------|-----------|------|---------|----------|----------|
| TinyLlama | 1.1B | 3GB | â­â­â­ | âš ï¸ | âœ… Open |
| Qwen2.5-1.5B | 1.5B | 4GB | â­â­â­ | âœ… | âœ… Open |
| Phi-2 | 2.7B | 6GB | â­â­â­â­ | âš ï¸ | âœ… Open |
| Qwen2-7B | 7B | 14GB | â­â­â­â­ | âœ…âœ… | âœ… Open |
| Qwen2.5-7B | 7B | 14GB | â­â­â­â­â­ | âœ…âœ… | âœ… Open |

---

## ğŸš€ Inizia Subito

1. **Riavvia backend**: `./start-dev.sh`
2. **Scegli modello** dalla lista
3. **Configura parametri**
4. **Avvia fine-tuning**!

ğŸ“– Guida completa: `LLM_MODELS_GUIDE.md`
