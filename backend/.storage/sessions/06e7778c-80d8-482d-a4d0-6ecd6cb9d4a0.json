{
  "id": "06e7778c-80d8-482d-a4d0-6ecd6cb9d4a0",
  "name": "New Workflow",
  "created_at": "2025-10-08T17:13:38.878779",
  "updated_at": "2025-10-08T17:17:34.447432",
  "workflow": {
    "name": "New Workflow",
    "nodes": [
      {
        "id": "llm.model_loader-1759936425125",
        "type": "customNode",
        "position": {
          "x": 0,
          "y": 504
        },
        "data": {
          "type": "llm.model_loader",
          "label": "LLM Model",
          "spec": {
            "type": "llm.model_loader",
            "label": "LLM Model",
            "category": "llm",
            "description": "Load pre-trained language model for chat and text generation",
            "inputs": [],
            "outputs": [
              {
                "name": "model",
                "type": "model",
                "label": "Loaded Model",
                "required": true,
                "description": null
              }
            ],
            "params": [
              {
                "name": "model_name",
                "type": "select",
                "label": "Model",
                "default": "microsoft/phi-2",
                "required": false,
                "description": "Pre-trained model to load. Phi-2 recommended for RTX 4090",
                "options": [
                  "microsoft/phi-2",
                  "Qwen/Qwen2.5-1.5B",
                  "Qwen/Qwen2.5-7B",
                  "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                  "HuggingFaceTB/SmolLM-1.7B",
                  "gpt2",
                  "distilgpt2"
                ],
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "use_4bit",
                "type": "boolean",
                "label": "Use 4-bit Quantization",
                "default": true,
                "required": false,
                "description": "Load model in 4-bit to save VRAM (requires bitsandbytes)",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "use_8bit",
                "type": "boolean",
                "label": "Use 8-bit Quantization",
                "default": false,
                "required": false,
                "description": "Load model in 8-bit (alternative to 4-bit)",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "device",
                "type": "select",
                "label": "Device",
                "default": "auto",
                "required": false,
                "description": "Device to load model on",
                "options": [
                  "auto",
                  "cuda",
                  "cpu"
                ],
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              }
            ],
            "cache_policy": "manual",
            "icon": "\ud83e\udd16",
            "color": "#6366F1"
          },
          "params": {
            "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "use_4bit": true,
            "use_8bit": false,
            "device": "auto"
          },
          "status": "success",
          "executionTime": 1.394277572631836
        },
        "width": 700,
        "height": 495,
        "selected": false,
        "dragging": false,
        "positionAbsolute": {
          "x": 0,
          "y": 504
        }
      },
      {
        "id": "llm.chat-1759936436526",
        "type": "customNode",
        "position": {
          "x": 984,
          "y": 552
        },
        "data": {
          "type": "llm.chat",
          "label": "Chat",
          "spec": {
            "type": "llm.chat",
            "label": "Chat",
            "category": "llm",
            "description": "Interactive chat interface with loaded LLM model",
            "inputs": [
              {
                "name": "model",
                "type": "model",
                "label": "LLM Model",
                "required": true,
                "description": null
              },
              {
                "name": "context",
                "type": "table",
                "label": "Context Data",
                "required": false,
                "description": null
              }
            ],
            "outputs": [
              {
                "name": "conversation",
                "type": "table",
                "label": "Chat History",
                "required": true,
                "description": null
              }
            ],
            "params": [
              {
                "name": "system_prompt",
                "type": "string",
                "label": "System Prompt",
                "default": "You are a helpful AI assistant.",
                "required": false,
                "description": "System instructions for the model",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "temperature",
                "type": "slider",
                "label": "Temperature",
                "default": 0.7,
                "required": false,
                "description": "Sampling temperature (higher = more creative)",
                "options": null,
                "min": 0.1,
                "max": 2,
                "step": 0.1,
                "accept": null,
                "language": null
              },
              {
                "name": "max_new_tokens",
                "type": "integer",
                "label": "Max New Tokens",
                "default": 512,
                "required": false,
                "description": "Maximum tokens to generate in response",
                "options": null,
                "min": 50,
                "max": 2048,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "top_p",
                "type": "slider",
                "label": "Top P",
                "default": 0.9,
                "required": false,
                "description": "Nucleus sampling threshold",
                "options": null,
                "min": 0.1,
                "max": 1,
                "step": 0.05,
                "accept": null,
                "language": null
              },
              {
                "name": "chat_history",
                "type": "string",
                "label": "Chat History (Internal)",
                "default": "[]",
                "required": false,
                "description": "Internal storage for conversation history",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              }
            ],
            "cache_policy": "never",
            "icon": "\ud83d\udcac",
            "color": "#8B5CF6"
          },
          "params": {
            "system_prompt": "You are a helpful AI assistant.",
            "temperature": 0.7,
            "max_new_tokens": 512,
            "top_p": 0.9,
            "chat_history": "[]"
          },
          "status": "success",
          "executionTime": 0.0005319118499755859
        },
        "width": 700,
        "height": 621,
        "selected": false,
        "positionAbsolute": {
          "x": 984,
          "y": 552
        },
        "dragging": false
      }
    ],
    "edges": [
      {
        "source": "llm.model_loader-1759936425125",
        "sourceHandle": "model",
        "target": "llm.chat-1759936436526",
        "targetHandle": "model",
        "id": "llm.model_loader-1759936425125-model-llm.chat-1759936436526-model",
        "data": {
          "targetCategory": "llm"
        },
        "className": "edge-llm"
      }
    ],
    "executionResults": {
      "llm.model_loader-1759936425125": {
        "outputs": {
          "model": {
            "model_id": "llm.model_loader-1759936425125_TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "quantization": "full",
            "total_params": 1100048384
          }
        },
        "metadata": {
          "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
          "total_params": 1100048384,
          "trainable_params": 1100048384,
          "quantization": "full",
          "device": "cuda:0",
          "model_id": "llm.model_loader-1759936425125_TinyLlama/TinyLlama-1.1B-Chat-v1.0"
        },
        "preview": {
          "type": "model_info",
          "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
          "parameters": "1,100,048,384",
          "quantization": "full"
        },
        "execution_time": 1.394277572631836
      },
      "llm.chat-1759936436526": {
        "outputs": {
          "conversation": {}
        },
        "metadata": {
          "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
          "system_prompt": "You are a helpful AI assistant.",
          "temperature": 0.7,
          "max_new_tokens": 512,
          "messages_count": 0
        },
        "preview": {
          "type": "chat",
          "model_name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
          "system_prompt": "You are a helpful AI assistant.",
          "conversation": [],
          "editable": true
        },
        "execution_time": 0.0005319118499755859
      }
    }
  }
}