{
  "id": "ab242324-5644-4538-b4c6-52f6f6cdf589",
  "name": "LLM",
  "created_at": "2025-10-03T14:39:29.680601",
  "updated_at": "2025-10-08T09:30:03.992517",
  "workflow": {
    "name": "LLM",
    "nodes": [
      {
        "id": "llm.dataset-1759497693611",
        "type": "customNode",
        "position": {
          "x": 720,
          "y": 120
        },
        "data": {
          "type": "llm.dataset",
          "label": "LLM Dataset",
          "spec": {
            "type": "llm.dataset",
            "label": "LLM Dataset",
            "category": "llm",
            "description": "Generate conversational/instructional dataset for LLM fine-tuning",
            "inputs": [],
            "outputs": [
              {
                "name": "dataset",
                "type": "table",
                "label": "Training Dataset",
                "required": true,
                "description": null
              }
            ],
            "params": [
              {
                "name": "format",
                "type": "select",
                "label": "Dataset Format",
                "default": "conversation",
                "required": false,
                "description": "Type of dataset to generate",
                "options": [
                  "instruction",
                  "conversation",
                  "completion"
                ],
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "n_samples",
                "type": "integer",
                "label": "Number of Samples",
                "default": 100,
                "required": false,
                "description": "Number of training examples",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "domain",
                "type": "select",
                "label": "Domain",
                "default": "general",
                "required": false,
                "description": "Domain for generated examples",
                "options": [
                  "general",
                  "customer_support",
                  "code",
                  "creative_writing",
                  "qa"
                ],
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "language",
                "type": "select",
                "label": "Language",
                "default": "italian",
                "required": false,
                "description": "Language for generated text",
                "options": [
                  "english",
                  "italian"
                ],
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "custom_prompt",
                "type": "string",
                "label": "Custom Prompt (Optional)",
                "default": "",
                "required": false,
                "description": "Add custom instructions for generation",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "seed",
                "type": "integer",
                "label": "Random Seed",
                "default": 42,
                "required": false,
                "description": "Seed for reproducibility",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              }
            ],
            "cache_policy": "manual",
            "icon": "\ud83d\udcac",
            "color": "#FF6B6B"
          },
          "params": {
            "format": "conversation",
            "n_samples": 100,
            "domain": "general",
            "language": "italian",
            "custom_prompt": "esempi di interazioni con un operatore di banca",
            "seed": 42
          },
          "status": "success",
          "executionTime": 0.0019307136535644531
        },
        "width": 300,
        "height": 633,
        "selected": false,
        "dragging": false,
        "positionAbsolute": {
          "x": 720,
          "y": 120
        }
      },
      {
        "id": "llm.finetune-1759823365419",
        "type": "customNode",
        "position": {
          "x": 1632,
          "y": -96
        },
        "data": {
          "type": "llm.finetune",
          "label": "Fine-tune LLM",
          "spec": {
            "type": "llm.finetune",
            "label": "Fine-tune LLM",
            "category": "llm",
            "description": "Fine-tune GPT-2 or DistilGPT-2 on custom dataset",
            "inputs": [
              {
                "name": "dataset",
                "type": "table",
                "label": "Training Dataset",
                "required": true,
                "description": null
              }
            ],
            "outputs": [
              {
                "name": "model",
                "type": "model",
                "label": "Fine-tuned Model",
                "required": true,
                "description": null
              },
              {
                "name": "metrics",
                "type": "metrics",
                "label": "Training Metrics",
                "required": true,
                "description": null
              },
              {
                "name": "loss_plot",
                "type": "params",
                "label": "Loss Plot",
                "required": true,
                "description": null
              },
              {
                "name": "loss_history",
                "type": "table",
                "label": "Loss History Table",
                "required": true,
                "description": null
              }
            ],
            "params": [
              {
                "name": "base_model",
                "type": "select",
                "label": "Base Model",
                "default": "microsoft/phi-2",
                "required": false,
                "description": "Pre-trained model to fine-tune. Phi-2 (2.7B) is recommended for RTX 4090",
                "options": [
                  "distilgpt2",
                  "gpt2",
                  "gpt2-medium",
                  "meta-llama/Llama-3.2-1B",
                  "meta-llama/Llama-3.2-3B",
                  "meta-llama/Llama-3.1-8B",
                  "mistralai/Mistral-7B-v0.3",
                  "mistralai/Mistral-7B-Instruct-v0.3",
                  "microsoft/phi-2",
                  "microsoft/Phi-3-mini-4k-instruct",
                  "microsoft/Phi-3.5-mini-instruct",
                  "google/gemma-2b",
                  "google/gemma-7b",
                  "google/gemma-2-9b",
                  "Qwen/Qwen2-1.5B",
                  "Qwen/Qwen2-7B",
                  "Qwen/Qwen2.5-7B",
                  "stabilityai/stablelm-2-1_6b",
                  "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                  "HuggingFaceTB/SmolLM-1.7B"
                ],
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "text_column",
                "type": "column",
                "label": "Text Column",
                "default": null,
                "required": true,
                "description": "Column with text to train on",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "epochs",
                "type": "integer",
                "label": "Epochs",
                "default": 3,
                "required": false,
                "description": "Number of training epochs",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "batch_size",
                "type": "integer",
                "label": "Batch Size",
                "default": 1,
                "required": false,
                "description": "Training batch size (use 1 for low memory)",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "gradient_accumulation_steps",
                "type": "integer",
                "label": "Gradient Accumulation Steps",
                "default": 4,
                "required": false,
                "description": "Accumulate gradients over N steps (simulates larger batch)",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "use_cpu",
                "type": "boolean",
                "label": "Use CPU Only",
                "default": false,
                "required": false,
                "description": "Train on CPU instead of GPU (slower but no memory issues)",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "fp16",
                "type": "boolean",
                "label": "Use FP16 (Half Precision)",
                "default": false,
                "required": false,
                "description": "Use 16-bit precision to save memory (requires GPU)",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "learning_rate",
                "type": "number",
                "label": "Learning Rate",
                "default": 5e-05,
                "required": false,
                "description": "Learning rate for training",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "max_length",
                "type": "integer",
                "label": "Max Length",
                "default": 64,
                "required": false,
                "description": "Maximum sequence length (lower = less memory)",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "max_samples",
                "type": "integer",
                "label": "Max Training Samples",
                "default": 0,
                "required": false,
                "description": "Limit training samples (0 = use all)",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              }
            ],
            "cache_policy": "manual",
            "icon": "\ud83c\udf93",
            "color": "#4ECDC4"
          },
          "params": {
            "base_model": "Qwen/Qwen2.5-7B",
            "text_column": "text",
            "epochs": 3,
            "batch_size": 1,
            "gradient_accumulation_steps": 4,
            "use_cpu": false,
            "fp16": false,
            "learning_rate": 5e-05,
            "max_length": 64,
            "max_samples": 0
          },
          "status": "error",
          "error": "Failed to fine-tune LLM: CUDA out of memory. Tried to allocate 26.00 MiB. GPU 0 has a total capacity of 23.51 GiB of which 39.31 MiB is free. Including non-PyTorch memory, this process has 22.18 GiB memory in use. Of the allocated memory 21.68 GiB is allocated by PyTorch, and 31.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
        },
        "width": 300,
        "height": 1104,
        "selected": true,
        "dragging": false,
        "positionAbsolute": {
          "x": 1632,
          "y": -96
        }
      },
      {
        "id": "llm.generate-1759823670317",
        "type": "customNode",
        "position": {
          "x": 2208,
          "y": 48
        },
        "data": {
          "type": "llm.generate",
          "label": "Text Generator",
          "spec": {
            "type": "llm.generate",
            "label": "Text Generator",
            "category": "llm",
            "description": "Generate text using fine-tuned language model",
            "inputs": [
              {
                "name": "model",
                "type": "model",
                "label": "Fine-tuned Model",
                "required": true,
                "description": null
              }
            ],
            "outputs": [
              {
                "name": "generated",
                "type": "table",
                "label": "Generated Texts",
                "required": true,
                "description": null
              }
            ],
            "params": [
              {
                "name": "prompt",
                "type": "string",
                "label": "Prompt",
                "default": "Once upon a time",
                "required": false,
                "description": "Starting text for generation",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "max_length",
                "type": "integer",
                "label": "Max Length",
                "default": 100,
                "required": false,
                "description": "Maximum tokens to generate",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              },
              {
                "name": "temperature",
                "type": "slider",
                "label": "Temperature",
                "default": 0.7,
                "required": false,
                "description": "Sampling temperature (higher = more creative)",
                "options": null,
                "min": 0.1,
                "max": 2,
                "step": 0.1,
                "accept": null,
                "language": null
              },
              {
                "name": "top_p",
                "type": "slider",
                "label": "Top P",
                "default": 0.9,
                "required": false,
                "description": "Nucleus sampling threshold",
                "options": null,
                "min": 0.1,
                "max": 1,
                "step": 0.05,
                "accept": null,
                "language": null
              },
              {
                "name": "num_return_sequences",
                "type": "integer",
                "label": "Number of Generations",
                "default": 3,
                "required": false,
                "description": "How many texts to generate",
                "options": null,
                "min": null,
                "max": null,
                "step": null,
                "accept": null,
                "language": null
              }
            ],
            "cache_policy": "manual",
            "icon": "\u2728",
            "color": "#95E1D3"
          },
          "params": {
            "prompt": "Once upon a time",
            "max_length": 100,
            "temperature": 0.7,
            "top_p": 0.9,
            "num_return_sequences": 3
          },
          "status": "error",
          "error": "Source port model not found in node llm.finetune-1759823365419"
        },
        "width": 300,
        "height": 621,
        "selected": false,
        "dragging": false,
        "positionAbsolute": {
          "x": 2208,
          "y": 48
        }
      }
    ],
    "edges": [
      {
        "source": "llm.dataset-1759497693611",
        "sourceHandle": "dataset",
        "target": "llm.finetune-1759823365419",
        "targetHandle": "dataset",
        "id": "llm.dataset-1759497693611-dataset-llm.finetune-1759823365419-dataset",
        "data": {
          "targetCategory": "llm"
        },
        "className": "edge-llm"
      },
      {
        "source": "llm.finetune-1759823365419",
        "sourceHandle": "model",
        "target": "llm.generate-1759823670317",
        "targetHandle": "model",
        "id": "llm.finetune-1759823365419-model-llm.generate-1759823670317-model",
        "data": {
          "targetCategory": "llm"
        },
        "className": "edge-llm"
      }
    ],
    "executionResults": {}
  }
}